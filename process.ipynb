{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据，生成一个大的npy文件\n",
    "导入os、numpy、PIL库，并确保tqdm库可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed for images and masks!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "\n",
    "# 定义输入图像和掩码目录\n",
    "image_dir = \"data/Img/\"\n",
    "mask_dir = \"data/Lab/\"\n",
    "\n",
    "# 创建输出目录\n",
    "output_image_dir = \"data/Img_\"\n",
    "output_mask_dir = \"data/Lab_\"\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# 获取所有图像和掩码文件\n",
    "image_files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "mask_files = glob.glob(os.path.join(mask_dir, \"*.png\"))\n",
    "\n",
    "# 检查图像和掩码的数量是否匹配\n",
    "if len(image_files) != len(mask_files):\n",
    "    raise ValueError(\"Number of images and masks do not match!\")\n",
    "\n",
    "# 添加自定义变换，每种变换都应用一次\n",
    "def augment_image_and_mask(image, mask, base_name):\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "    \n",
    "    # 1. 垂直翻转\n",
    "    flipped_img = image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    flipped_mask = mask.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    augmented_images.append((flipped_img, flipped_mask, \"_flip\"))\n",
    "\n",
    "    # 2. 对比度增强\n",
    "    contrast_factor = np.random.uniform(1.0, 1.5)\n",
    "    contrasted_img = ImageEnhance.Contrast(image).enhance(contrast_factor)\n",
    "    augmented_images.append((contrasted_img, mask, \"_contrast\"))\n",
    "\n",
    "    # 3. 亮度增强\n",
    "    brightness_factor = np.random.uniform(0.5, 1.5)\n",
    "    brightened_img = ImageEnhance.Brightness(image).enhance(brightness_factor)\n",
    "    augmented_images.append((brightened_img, mask, \"_brightness\"))\n",
    "\n",
    "    # 4. 放大和裁切\n",
    "    scale_factor = 1.2\n",
    "    new_size = (int(image.size[0] * scale_factor), int(image.size[1] * scale_factor))\n",
    "    resized_img = image.resize(new_size)\n",
    "    resized_mask = mask.resize(new_size)\n",
    "    augmented_images.append((resized_img, resized_mask, \"_resize\"))\n",
    "\n",
    "    # 5. 裁切85%\n",
    "    cropped_img = image.crop((0, 0, int(image.size[0] * 0.85), int(image.size[1] * 0.85)))\n",
    "    cropped_mask = mask.crop((0, 0, int(mask.size[0] * 0.85), int(mask.size[1] * 0.85)))\n",
    "    augmented_images.append((cropped_img, cropped_mask, \"_crop\"))\n",
    "\n",
    "    # 6. 添加椒盐噪声\n",
    "    image_np = np.array(image)\n",
    "    height, width, channel = image_np.shape\n",
    "    num_salt = 500  # 椒盐噪声的数量\n",
    "    for i in range(num_salt):\n",
    "        x = np.random.randint(0, height)\n",
    "        y = np.random.randint(0, width)\n",
    "        image_np[x, y, :] = 255  # 椒盐噪声\n",
    "    noisy_img = Image.fromarray(image_np)\n",
    "    augmented_images.append((noisy_img, mask, \"_salt_noise\"))\n",
    "\n",
    "    # 7. 最终调整为256x256\n",
    "    resized_200_img = image.resize((200, 200))\n",
    "    resized_200_mask = mask.resize((200, 200))\n",
    "    augmented_images.append((resized_200_img, resized_200_mask, \"_final_resize\"))\n",
    "\n",
    "    # 保存所有增强后的图像和掩码\n",
    "    for img, msk, suffix in augmented_images:\n",
    "        new_img_name = base_name + suffix + \".jpg\"\n",
    "        new_mask_name = base_name + suffix + \".png\"\n",
    "\n",
    "        img.save(os.path.join(output_image_dir, new_img_name))\n",
    "        msk.save(os.path.join(output_mask_dir, new_mask_name))\n",
    "\n",
    "# 逐一处理图像和掩码\n",
    "for img_file, mask_file in zip(image_files, mask_files):\n",
    "    img = Image.open(img_file).convert('RGB')\n",
    "    mask = Image.open(mask_file).convert('L')\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "    augment_image_and_mask(img, mask, base_name)\n",
    "\n",
    "print(\"Data augmentation completed for images and masks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件夹路径\n",
    "image_dir = 'data/Img'\n",
    "mask_dir = 'data/Lab'\n",
    "\n",
    "# 获取文件列表\n",
    "images = sorted(os.listdir(image_dir))\n",
    "masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# 确保文件对应\n",
    "assert len(images) == len(masks), \"图片和掩码数量不一致\"\n",
    "\n",
    "# 完整路径\n",
    "image_paths = [os.path.join(image_dir, img) for img in images]\n",
    "mask_paths = [os.path.join(mask_dir, msk) for msk in masks]\n",
    "\n",
    "# 分割数据集\n",
    "train_imgs, test_imgs, train_masks, test_masks = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "def load_images(paths):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.transpose(img_array, (2, 0, 1))  # 将通道轴移到最前面\n",
    "        data.append(img_array)\n",
    "    return np.array(data)\n",
    "\n",
    "def load_masks(paths):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        data.append(np.array(img))\n",
    "    return np.array(data)\n",
    "\n",
    "# 加载并保存训练集\n",
    "train_images = load_images(train_imgs)\n",
    "train_masks = load_masks(train_masks)\n",
    "np.save('data/train_images.npy', train_images)\n",
    "np.save('data/train_masks.npy', train_masks)\n",
    "\n",
    "# 加载并保存测试集\n",
    "test_images = load_images(test_imgs)\n",
    "test_masks = load_masks(test_masks)\n",
    "np.save('data/test_images.npy', test_images)\n",
    "np.save('data/test_masks.npy', test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{'work_dir': './work_dir/', 'config': './config/train.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'feeder': 'feeder.Feeder', 'num_worker': 8, 'train_feeder_args': {'mode': 'train', 'data_path': './data/train_images.npy', 'label_path': './data/train_masks.npy'}, 'test_feeder_args': {'mode': 'test', 'data_path': './data/test_images.npy', 'label_path': './data/test_masks.npy'}, 'model': 'net.self_net', 'model_args': {'num_class': 4, 'channel': 3}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.01, 'step': [10, 50], 'device': 0, 'optimizer': 'Adam', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 16, 'start_epoch': 0, 'num_epoch': 150, 'weight_decay': 0.0005}\n",
      "\n",
      "Parameters: 147300\n",
      "Epoch:[1/150]\n",
      "100%|███████████████████████████████████████████| 40/40 [00:03<00:00, 12.27it/s]\n",
      "Train iou: [0.00216127 0.26110874 0.0246982 ] Train miou: 0.09598940342272322 Mean loss: 0.6753625631332397 LR: [0.0095]\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]torch.Size([16, 3, 200, 200])\n",
      "Unsupported operator aten::_convolution_mode encountered 19 time(s)\n",
      "Unsupported operator aten::add encountered 18 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
      "Unsupported operator aten::gelu encountered 7 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "class0.d1.pw2, class0.d2.pw2, class0.d3.pw2\n",
      "FLOPs: 4779520000\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdch/workspace/Segmodel/main.py\", line 222, in <module>\n",
      "    processor.start()\n",
      "  File \"/home/pdch/workspace/Segmodel/main.py\", line 198, in start\n",
      "    self.train()\n",
      "  File \"/home/pdch/workspace/Segmodel/main.py\", line 145, in train\n",
      "    self.eval()\n",
      "  File \"/home/pdch/workspace/Segmodel/main.py\", line 174, in eval\n",
      "    assert 1 == 0\n",
      "AssertionError\n",
      "Exception ignored in atexit callback: <function FileWriter.__init__.<locals>.cleanup at 0x7f404c1b77e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/site-packages/tensorboardX/writer.py\", line 108, in cleanup\n",
      "    self.event_writer.close()\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/site-packages/tensorboardX/event_file_writer.py\", line 157, in close\n",
      "    self._worker.stop()\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/site-packages/tensorboardX/event_file_writer.py\", line 188, in stop\n",
      "    self._queue.put(self._shutdown_signal)\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/multiprocessing/queues.py\", line 94, in put\n",
      "    self._start_thread()\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/multiprocessing/queues.py\", line 177, in _start_thread\n",
      "    self._thread.start()\n",
      "  File \"/home/pdch/miniforge3/envs/torch/lib/python3.12/threading.py\", line 971, in start\n",
      "    _start_new_thread(self._bootstrap, ())\n",
      "RuntimeError: can't create new thread at interpreter shutdown\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清理log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf work_dir/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
